{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster Transformers with Pytorch and TVM\n",
    "\n",
    "credit: https://github.com/t-vi/pytorch-tvmisc/blob/master/transformers-pytorch-tvm/bert-tvm.ipynb\n",
    "\n",
    "a tutorial by Thomas Viehmann <tv@lernapparat.de>\n",
    "\n",
    "\n",
    "Acknowledgement & Disclosure: The creation of this tutorial was sponsored by AMD. Thank you!\n",
    "\n",
    "Some of the most intriguing applications of Artificial Intelligence have been in Natural Language Processing.\n",
    "Models like BERT or GPT-2 and their variants can seemingly grasp enough of a text to continue it in a way that needs a second look to recognize as gibberish.\n",
    "\n",
    "These models belong to a class of neural network architectures called *Transformers*. One of the favourite libraries implementing them is the [HuggingFace transformers library](https://github.com/huggingface/transformers/).\n",
    "\n",
    "But, in contrast to convolutional models or LSTMs where we have heavily optimized implementations, this is not as much the case for transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla T4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I sometimes need to choose PyTorch...\n",
    "import inspect\n",
    "import sys\n",
    "import torch\n",
    "import torch.utils.dlpack\n",
    "\n",
    "# import TVM\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ[\"PATH\"]=\"/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin\"\n",
    "\n",
    "tvm_root = '/home/ubuntu/workspace/tvm/'\n",
    "tvm_paths = [os.path.join(tvm_root, p) for p in ['python', 'topi/python', 'nnvm/python']]\n",
    "os.environ['PYTHONPATH'] = ':'.join([os.environ.get('PYTHONPATH', '')] + tvm_paths)\n",
    "for p in tvm_paths:\n",
    "    sys.path.insert(0, p)\n",
    "    \n",
    "\n",
    "import tvm\n",
    "import tvm.relay\n",
    "\n",
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://opentuna.cn/pypi/web/simple/\n",
      "Requirement already satisfied: regex in /home/ubuntu/anaconda3/lib/python3.7/site-packages (2020.7.14)\n",
      "Requirement already satisfied: sacremoses in /home/ubuntu/anaconda3/lib/python3.7/site-packages (0.0.43)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/anaconda3/lib/python3.7/site-packages (from sacremoses) (4.44.1)\n",
      "Requirement already satisfied: joblib in /home/ubuntu/anaconda3/lib/python3.7/site-packages (from sacremoses) (0.14.1)\n",
      "Requirement already satisfied: click in /home/ubuntu/anaconda3/lib/python3.7/site-packages (from sacremoses) (7.1.1)\n",
      "Requirement already satisfied: six in /home/ubuntu/anaconda3/lib/python3.7/site-packages (from sacremoses) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -i https://opentuna.cn/pypi/web/simple/ regex sacremoses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpfully, transformers supports tracing their model with the PyTorch JIT. We use their [tutorial on it](https://huggingface.co/transformers/torchscript.html), the following is copied straight from the tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.1.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "from transformers import BertModel, BertTokenizer, BertConfig\n",
    "import numpy\n",
    "\n",
    "import torch\n",
    "\n",
    "enc = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenizing input text\n",
    "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
    "tokenized_text = enc.tokenize(text)\n",
    "\n",
    "# Masking one of the input tokens\n",
    "masked_index = 8\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "indexed_tokens = enc.convert_tokens_to_ids(tokenized_text)\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Creating a dummy input\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "dummy_input = [tokens_tensor, segments_tensors]\n",
    "\n",
    "# If you are instantiating the model with `from_pretrained` you can also easily set the TorchScript flag\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\", torchscript=True)\n",
    "\n",
    "model.eval()\n",
    "for p in model.parameters():\n",
    "    p.requires_grad_(False)\n",
    "\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can trace our model. As we want to do inference, we impose evaluation mode and not requiring gradients for the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/transformers/modeling_bert.py:201: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  position_ids = self.position_ids[:, :seq_length]\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/transformers/modeling_utils.py:1570: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  input_tensor.shape == tensor_shape for input_tensor in input_tensors\n"
     ]
    }
   ],
   "source": [
    "# Creating the trace\n",
    "traced_model = torch.jit.trace(model, [tokens_tensor, segments_tensors])\n",
    "traced_model.eval()\n",
    "for p in traced_model.parameters():\n",
    "    p.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us run try our traced model on the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()\n",
    "tt_c = tokens_tensor.cuda()\n",
    "st_c = segments_tensors.cuda()\n",
    "res_pt = model(tt_c, st_c)\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked, but is it fast? Let's run it 100 times and see.\n",
    "When timing CUDA models, it's always good to do some \"warm-up\", running the model before the measurement, and we need to be sure to synchronize before the start and end of the timing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed: 922.147 ms\n"
     ]
    }
   ],
   "source": [
    "def y():\n",
    "    for i in range(100):\n",
    "        model(tt_c, st_c)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "import time\n",
    "tic = time.time()\n",
    "y()\n",
    "toc = time.time()\n",
    "print(\"elapsed: {:.03f} ms\".format((toc - tic)*1000.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around 0.91-0.98 seconds for 100 runs means 9.1-9.8ms per run. That's not too bad.\n",
    "\n",
    "But let us see if TVM can help us to get faster. Let us convert our model to TVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('input_ids', [1, 14]), ('attention_mask', [1, 14])]\n"
     ]
    }
   ],
   "source": [
    "shape_list = [(i.debugName().split('.')[0], i.type().sizes()) for i in  list(traced_model.graph.inputs())[1:]]\n",
    "print(shape_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 0 -\n",
      "- 0 -\n"
     ]
    }
   ],
   "source": [
    "print(\"- 0 -\", flush=True)\n",
    "mod_bert, params_bert = tvm.relay.frontend.pytorch.from_pytorch(traced_model,\n",
    "                        shape_list, default_dtype=\"float32\")\n",
    "print(\"- 0 -\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That went well! (Be sure to use the TVM model from my git branch.) We can now build and run it. Building follows the standard TVM recipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"cuda -model=t4\"\n",
    "ctx = tvm.context(target, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_host = 'llvm'\n",
    "\n",
    "tt_a = tvm.nd.array(tokens_tensor.numpy(), ctx)\n",
    "st_a = tvm.nd.array(segments_tensors.numpy(), ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 1 -\n",
      "download failed due to URLError(ConnectionRefusedError(111, 'Connection refused')), retrying, 2 attempts left\n",
      "download failed due to URLError(ConnectionRefusedError(111, 'Connection refused')), retrying, 1 attempt left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to download tophub package for cuda: <urlopen error [Errno 111] Connection refused>\n",
      "WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -model=t4 -thread_warp_size=32, workload=('batch_matmul.cuda', ('TENSOR', (1, 14, 3072), 'float32'), ('TENSOR', (1, 768, 3072), 'float32')). A fallback configuration is used, which may bring great performance regression.\n",
      "WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -model=t4 -thread_warp_size=32, workload=('batch_matmul.cuda', ('TENSOR', (1, 14, 768), 'float32'), ('TENSOR', (1, 3072, 768), 'float32')). A fallback configuration is used, which may bring great performance regression.\n",
      "WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -model=t4 -thread_warp_size=32, workload=('batch_matmul.cuda', ('TENSOR', (1, 14, 768), 'float32'), ('TENSOR', (1, 768, 768), 'float32')). A fallback configuration is used, which may bring great performance regression.\n",
      "WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -model=t4 -thread_warp_size=32, workload=('batch_matmul.cuda', ('TENSOR', (12, 14, 14), 'float32'), ('TENSOR', (12, 64, 14), 'float32')). A fallback configuration is used, which may bring great performance regression.\n",
      "WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -model=t4 -thread_warp_size=32, workload=('batch_matmul.cuda', ('TENSOR', (12, 14, 64), 'float32'), ('TENSOR', (12, 14, 64), 'float32')). A fallback configuration is used, which may bring great performance regression.\n",
      "WARNING:autotvm:Cannot find config for target=cuda -keys=cuda,gpu -max_num_threads=1024 -model=t4 -thread_warp_size=32, workload=('dense_small_batch.cuda', ('TENSOR', (1, 768), 'float32'), ('TENSOR', (768, 768), 'float32'), None, 'float32'). A fallback configuration is used, which may bring great performance regression.\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: legacy graph runtime behaviour of producing json / lib / params will be removed in the next release \n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 2 -\n"
     ]
    }
   ],
   "source": [
    "print(\"- 1 -\")\n",
    "tvm.relay.backend.compile_engine.get().clear() # just to be sure, see https://github.com/apache/incubator-tvm/pull/5724\n",
    "\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "        graph, lib, params = tvm.relay.build(mod_bert,\n",
    "                                     target=target,\n",
    "                                     target_host=target_host,\n",
    "                                     params=params_bert)\n",
    "module = tvm.contrib.graph_runtime.create(graph, lib, ctx)\n",
    "print(\"- 2 -\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh oh, _may bring great performance regression_. Let's see. We run the module:\n",
    "\n",
    "Let us run the model and see if the outputs match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.6293945e-06 8.34465e-07\n"
     ]
    }
   ],
   "source": [
    "module.set_input(\"input_ids\", tt_a)\n",
    "module.set_input(\"attention_mask\", st_a)\n",
    "module.set_input(**params)\n",
    "module.run()\n",
    "o0 = module.get_output(0)\n",
    "o1 = module.get_output(1)\n",
    "print(numpy.abs((res_pt[0].cpu().numpy() - o0.asnumpy())).max(), \n",
    "      numpy.abs((res_pt[1].cpu().numpy() - o1.asnumpy())).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. Remember that we're computing in float32, so $10^{-6}$ish is a good result. Now that we know it gets the correct result, let us see what the speed is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 elapsed: 2040.782 ms\n"
     ]
    }
   ],
   "source": [
    "def x():\n",
    "    for i in range(100):\n",
    "        module.run()\n",
    "    ctx.sync()\n",
    "tic = time.time()\n",
    "x()\n",
    "toc = time.time()\n",
    "print(\"0 elapsed: {:.03f} ms\".format((toc - tic)*1000.0))\n",
    "# get_ipython().run_line_magic('timeit', 'x()')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch, 20 ms per run of the model. That's slow indeed. But the warning said that is was because it could not find (tuned) configurations. Let us then tune the tasks.\n",
    "We extract the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...100%, 0.40 MB, 368 KB/s, 1 seconds passed\n",
      "[Task(func_name=batch_matmul.cuda, args=(('TENSOR', (1, 14, 3072), 'float32'), ('TENSOR', (1, 768, 3072), 'float32')), kwargs={}, workload=('batch_matmul.cuda', ('TENSOR', (1, 14, 3072), 'float32'), ('TENSOR', (1, 768, 3072), 'float32'))), Task(func_name=batch_matmul.cuda, args=(('TENSOR', (1, 14, 768), 'float32'), ('TENSOR', (1, 3072, 768), 'float32')), kwargs={}, workload=('batch_matmul.cuda', ('TENSOR', (1, 14, 768), 'float32'), ('TENSOR', (1, 3072, 768), 'float32'))), Task(func_name=batch_matmul.cuda, args=(('TENSOR', (1, 14, 768), 'float32'), ('TENSOR', (1, 768, 768), 'float32')), kwargs={}, workload=('batch_matmul.cuda', ('TENSOR', (1, 14, 768), 'float32'), ('TENSOR', (1, 768, 768), 'float32'))), Task(func_name=batch_matmul.cuda, args=(('TENSOR', (12, 14, 14), 'float32'), ('TENSOR', (12, 64, 14), 'float32')), kwargs={}, workload=('batch_matmul.cuda', ('TENSOR', (12, 14, 14), 'float32'), ('TENSOR', (12, 64, 14), 'float32'))), Task(func_name=batch_matmul.cuda, args=(('TENSOR', (12, 14, 64), 'float32'), ('TENSOR', (12, 14, 64), 'float32')), kwargs={}, workload=('batch_matmul.cuda', ('TENSOR', (12, 14, 64), 'float32'), ('TENSOR', (12, 14, 64), 'float32'))), Task(func_name=dense_small_batch.cuda, args=(('TENSOR', (1, 768), 'float32'), ('TENSOR', (768, 768), 'float32'), None, 'float32'), kwargs={}, workload=('dense_small_batch.cuda', ('TENSOR', (1, 768), 'float32'), ('TENSOR', (768, 768), 'float32'), None, 'float32'))]\n"
     ]
    }
   ],
   "source": [
    "tasks = tvm.autotvm.task.extract_from_program(mod_bert[\"main\"], target=target, params=params)\n",
    "print(tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so we have are our tasks that we need to be able to perform fast.\n",
    "\n",
    "Below is the corresponding tuning. We have set `n_trial` to 20 here for you to play along. For serious tuning, you need to put this to 2000 steps. Each task than takes about 1-2 hours (on my computer).\n",
    "\n",
    "As I wanted this to be runnable from Jupyter, I'm doing a bit of a dance with threading and the tornado IOLoop module. In a regular script, you would only have the call to `tuner.tune` between _do tuning_ and _done tuning_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_filename = 'bert-tuning.stage1.log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task  1/ 6]  Current/Best:   95.46/ 288.23 GFLOPS | Progress: (18/18) | 25.22 s Done.\n",
      "[Task  2/ 6]  Current/Best:   88.12/  99.91 GFLOPS | Progress: (20/20) | 24.88 s Done.\n",
      "[Task  3/ 6]  Current/Best:  100.60/ 100.60 GFLOPS | Progress: (20/20) | 24.81 s Done.\n",
      "[Task  4/ 6]  Current/Best:  613.18/ 712.88 GFLOPS | Progress: (20/20) | 27.42 s Done.\n",
      "[Task  5/ 6]  Current/Best:  891.24/ 938.43 GFLOPS | Progress: (20/20) | 22.13 s Done.\n",
      "[Task  6/ 6]  Current/Best:  578.87/ 578.87 GFLOPS | Progress: (20/20) | 22.26 s Done.\n"
     ]
    }
   ],
   "source": [
    "n_trial = 20  # for real tuning, make this 2000!\n",
    "\n",
    "def do_tune(tasks, log_filename):\n",
    "    tmp_log_file = log_filename + \".tmp\"\n",
    "    for i, tsk in enumerate(reversed(tasks)):\n",
    "        prefix = \"[Task %2d/%2d] \" %(i+1, len(tasks))\n",
    "\n",
    "        # we use threading and tornado here to work around TVM and Jupyter colliding over IOLoops\n",
    "        # In a regular python command line, you should be able to just call the tuner...\n",
    "        import threading \n",
    "        import tornado\n",
    "\n",
    "        # create tuner\n",
    "        tuner = tvm.autotvm.tuner.XGBTuner(tsk, loss_type='rank')\n",
    "        if os.path.isfile(tmp_log_file):\n",
    "            tuner.load_history(tvm.autotvm.record.load_from_file(tmp_log_file))\n",
    "\n",
    "        # do tuning\n",
    "        tsk_trial = min(n_trial, len(tsk.config_space))\n",
    "        def tune_task_fn():\n",
    "            iol = tornado.ioloop.IOLoop()  # we need an event loop\n",
    "            tuner.tune(\n",
    "                n_trial=n_trial,\n",
    "                early_stopping=600,\n",
    "                measure_option=tvm.autotvm.measure_option(\n",
    "                    builder=tvm.autotvm.LocalBuilder(timeout=10),\n",
    "                    runner=tvm.autotvm.LocalRunner(number=20, repeat=3, timeout=4, min_repeat_ms=150)),\n",
    "                callbacks=[\n",
    "                    tvm.autotvm.callback.progress_bar(tsk_trial, prefix=prefix),\n",
    "                    tvm.autotvm.callback.log_to_file(tmp_log_file)\n",
    "                ])\n",
    "\n",
    "        tuning_thread = threading.Thread(target=tune_task_fn)  # create a thread start it and wait on it\n",
    "        tuning_thread.start()\n",
    "        tuning_thread.join()\n",
    "        # done tuning, on to the next task\n",
    "\n",
    "    # pick best records to a cache file\n",
    "    tvm.autotvm.record.pick_best(tmp_log_file, log_filename)\n",
    "\n",
    "do_tune(tasks, log_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we can again build the model, this time with the new configuration. This time we should see no comments about missing configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: legacy graph runtime behaviour of producing json / lib / params will be removed in the next release \n",
      "  \n"
     ]
    }
   ],
   "source": [
    "tvm.relay.backend.compile_engine.get().clear()\n",
    "\n",
    "with tvm.autotvm.apply_history_best(log_filename):\n",
    "    with tvm.transform.PassContext(opt_level=3):\n",
    "        graph, lib, params = tvm.relay.build(mod_bert,\n",
    "                                     target=target,\n",
    "                                     target_host=target_host,\n",
    "                                     params=params_bert)\n",
    "module = tvm.contrib.graph_runtime.create(graph, lib, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.6293945e-06 8.34465e-07\n"
     ]
    }
   ],
   "source": [
    "module.set_input(\"input_ids\", tt_a)\n",
    "module.set_input(\"attention_mask\", st_a)\n",
    "module.set_input(**params)\n",
    "module.run()\n",
    "o0 = module.get_output(0)\n",
    "o1 = module.get_output(1)\n",
    "print(numpy.abs((res_pt[0].cpu().numpy() - o0.asnumpy())).max(), \n",
    "      numpy.abs((res_pt[1].cpu().numpy() - o1.asnumpy())).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the speed improved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tvm elapsed: 584.373 ms\n"
     ]
    }
   ],
   "source": [
    "def x():\n",
    "    for i in range(100):\n",
    "        module.run()\n",
    "    ctx.sync()\n",
    "tic = time.time()\n",
    "x()\n",
    "toc = time.time()\n",
    "print(\"tvm elapsed: {:.03f} ms\".format((toc - tic)*1000.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's in the region of 5.5-7ms per run. That's faster comparing to PyTorch. This is what we get from this very elementary optimization of our operators. We can push it a little further, though."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
